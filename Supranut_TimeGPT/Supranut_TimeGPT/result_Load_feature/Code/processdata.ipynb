{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc8a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Happy Forecasting! :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nixtla import NixtlaClient\n",
    "import pandas as pd\n",
    "nixtla_client = NixtlaClient(\n",
    "    api_key='nixak-NUbao7AfDoqbzMPUCOBd0OJbUGsODIvE2iukG1MZAHH6UqbJ07Ea5CoUOIoKZ0pn50KDcwaWaBy0M9IH'\n",
    ") #this is my API you can change it.\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "solar_df = pd.read_csv(\"pv_8kW_5minresample_concat_impute.csv\", parse_dates=[\"Datetime\"], index_col=\"Datetime\")\n",
    "#solar_df = pd.read_csv(\"load_feature_5minresample_edited.csv\", parse_dates=[\"Datetime\"], index_col=\"Datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "c:\\Users\\petch\\ana\\Lib\\site-packages\\nixtla\\nixtla_client.py:394: UserWarning: `df` contains the following exogenous features: ['CI_CM_imputed', 'CI_RGB_imputed'], but they were not found in `X_df` nor declared in `hist_exog_list`. They will be ignored.\n",
      "  warnings.warn(\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Querying model metadata...\n",
      "WARNING:nixtla.nixtla_client:The specified horizon \"h\" exceeds the model horizon, this may lead to less accurate forecasts. Please consider using a smaller horizon.\n",
      "INFO:nixtla.nixtla_client:Using future exogenous features: ['Iclr_imputed', 'Inwp_imputed', 'Tnwp_imputed']\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "c:\\Users\\petch\\ana\\Lib\\site-packages\\nixtla\\nixtla_client.py:394: UserWarning: `df` contains the following exogenous features: ['CI_CM_imputed', 'CI_RGB_imputed'], but they were not found in `X_df` nor declared in `hist_exog_list`. They will be ignored.\n",
      "  warnings.warn(\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "WARNING:nixtla.nixtla_client:The specified horizon \"h\" exceeds the model horizon, this may lead to less accurate forecasts. Please consider using a smaller horizon.\n",
      "INFO:nixtla.nixtla_client:Using future exogenous features: ['Iclr_imputed', 'Inwp_imputed', 'Tnwp_imputed']\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#with cloud\n",
    "\n",
    "target_col = 'Irradiance (W/m2)_imputed'\n",
    "#target_col = 'Ltot_impute (kW)'\n",
    "lag_feature = ['Iclr_imputed', 'Inwp_imputed', 'Tnwp_imputed']\n",
    "#lag_feature = ['Tnwp' , 'lab_hour','lab_day_machine','lab_day_power','is_work_day']\n",
    "future_exogs_cols = ['Iclr_imputed', 'Inwp_imputed', 'Tnwp_imputed']\n",
    "#future_exogs_cols = ['Tnwp' , 'lab_hour','lab_day_machine','lab_day_power','is_work_day']\n",
    "\n",
    "start_date = pd.Timestamp(\"2024-01-01\")\n",
    "end_date = pd.Timestamp(\"2024-12-28\")\n",
    "\n",
    "current_date = start_date\n",
    "forecast_results = []  # เก็บผล forecast แต่ละวัน\n",
    "\n",
    "while current_date <= end_date:\n",
    "    # 1️⃣ historical 1 วัน\n",
    "    historical_range = pd.date_range(\n",
    "        start=current_date, # lag 1,3,7 เเก้ตรงนี้\n",
    "        end=current_date + pd.Timedelta(hours=23, minutes=55),\n",
    "        freq=\"5min\"\n",
    "    )\n",
    "\n",
    "    # 2️⃣ future 3 วัน\n",
    "    future_start = current_date + pd.Timedelta(days=1)\n",
    "    future_end = current_date + pd.Timedelta(days=3, hours=23, minutes=55)\n",
    "    future_range = pd.date_range(start=future_start, end=future_end, freq=\"5min\")\n",
    "    \n",
    "    n_forecasts = len(future_range)  # จำนวนจุดที่จะ forecast\n",
    "\n",
    "    # 3️⃣ เตรียม DataFrame\n",
    "    sliced_solar_df = (\n",
    "        solar_df.loc[historical_range]\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'Datetime'})[['Datetime', target_col] + lag_feature]\n",
    "    )\n",
    "\n",
    "    sliced_solar_future_exog = (\n",
    "        solar_df.loc[future_range]\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'Datetime'})[['Datetime'] + future_exogs_cols]\n",
    "    )\n",
    "\n",
    "    # 4️⃣ forecast ด้วย TimeGPT\n",
    "    timegpt_solar_result = nixtla_client.forecast(\n",
    "        df=sliced_solar_df,\n",
    "        h=n_forecasts,\n",
    "        X_df=sliced_solar_future_exog,\n",
    "        freq='5min',\n",
    "        time_col='Datetime',\n",
    "        target_col=target_col,\n",
    "    )\n",
    "\n",
    "    # # # 5️⃣ เก็บผลลัพธ์พร้อมวันที่อ้างอิง\n",
    "    forecast_results.append({\n",
    "        'base_date': current_date,\n",
    "        'forecast': timegpt_solar_result\n",
    "    })\n",
    "\n",
    "    # 6️⃣ ไปวันถัดไป\n",
    "    current_date += pd.Timedelta(days=1)\n",
    "    # current_date += pd.Timedelta(minutes= 5 ) ถ้า จะเอา roll 5 mins\n",
    "\n",
    "# print(f\"สร้าง forecast ครบ {len(forecast_results)} วัน (1 วันต่อรอบ)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = pd.Timestamp(\"2024-01-01\") #เเก้ไขได้\n",
    "ed = pd.Timestamp(\"2024-01-02\") #เเก้ไขได้\n",
    "cur_date = st\n",
    "count = 1\n",
    "diction = dict()\n",
    "while cur_date <= ed:\n",
    "    if count > 363:\n",
    "        break\n",
    "    select_for = None\n",
    "    for t in forecast_results:\n",
    "        if t['base_date'] == cur_date:\n",
    "            select_for = t['forecast']\n",
    "            break\n",
    "    period = pd.date_range(\n",
    "        start=cur_date,\n",
    "        end=cur_date + pd.Timedelta(hours=23, minutes=55),\n",
    "        freq=\"5min\"\n",
    "    )\n",
    "    \n",
    "    period_df = (\n",
    "        solar_df.loc[period]\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'Datetime'})[['Datetime', target_col] + future_exogs_cols]\n",
    "    )\n",
    "    key = period_df.tail(1)[\"Datetime\"].iloc[0]  # ใช้ Timestamp เดี่ยว\n",
    "    diction[key] = forecast_results[count-1]['forecast'].values  # .values() เป็น numpy array\n",
    "\n",
    "    cur_date += pd.Timedelta(days=1)\n",
    "    count += 1\n",
    "    last_row_of_day = period_df.tail(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "execution = pd.DataFrame()\n",
    "\n",
    "for key, arr in diction.items():\n",
    "    # ดึงค่าพยากรณ์\n",
    "    values = [v for _, v in arr]\n",
    "    horizon_cols = [f\"I_hat(t+{i+1}|t)\" for i in range(len(values))]\n",
    "    #horizon_cols = [f\"Load_hat(t+{i+1}|t)\" for i in range(len(values))] #สำหรับ Load\n",
    "\n",
    "    # สร้าง DataFrame แถวเดียว\n",
    "    temp_df = pd.DataFrame([values], columns=horizon_cols) #สร้าง col เปล่าตามตัวเเปรใน horizon\n",
    "\n",
    "    # --- insert Datetime และคอลัมน์เปล่า I(t) ---\n",
    "    temp_df.insert(0, \"I(t)\", np.nan)   # เพิ่มคอลัมน์เปล่า I(t)\n",
    "    #temp_df.insert(0, \"Load(t)\", np.nan) #columns Load(t)\n",
    "    temp_df.insert(0, \"Datetime\", key)  \n",
    "\n",
    "    # concat\n",
    "    execution = pd.concat([execution, temp_df], ignore_index=True)\n",
    "\n",
    "# เรียงตาม Datetime\n",
    "execution = execution.sort_values(\"Datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14924a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_csv(\"pv_8kW_5minresample_concat_impute.csv\") # เอามา merge รวมกับ dataframe\n",
    "#old_data = pd.read_csv(\"JSS_proj\\load_feature_5minresample_edited.csv\")\n",
    "old_data[\"Datetime\"] = pd.to_datetime(old_data[\"Datetime\"])\n",
    "# Merge เพื่อนำค่า I(t)\n",
    "execution = execution.merge(\n",
    "    old_data[['Datetime', 'Irradiance (W/m2)_imputed']],\n",
    "    on='Datetime',\n",
    "    how='left'\n",
    ")\n",
    "# execution = execution.merge(                      #สำหรับ Load\n",
    "#     old_data[['Datetime', 'Ltot_impute (kW)']],\n",
    "#     on='Datetime',\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "execution['I(t)'] = execution['Irradiance (W/m2)_imputed']\n",
    "#execution[\"Load(t)\"] = execution['Ltot_impute (kW)']\n",
    "\n",
    "execution = execution.drop(columns=['Irradiance (W/m2)_imputed'])\n",
    "# execution = execution.drop(columns=['Ltot_impute (kW)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a442d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution.to_csv(\"TimeGPT_lag1day(new_ver).csv\") #สร้างไฟล์ base on execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Convert for Solar\n",
    "STEP_MIN = 5  # 1 step = 5 นาที\n",
    "\n",
    "\n",
    "input_data_converse = pd.read_csv(\"TimeGPTlag7daySolar.csv\", parse_dates=[\"Datetime\"])\n",
    "e = pd.read_csv(\"pv_8kW_5minresample_concat_impute.csv\")\n",
    "e[\"Datetime\"] = pd.to_datetime(e[\"Datetime\"])\n",
    "\n",
    "# -------------------------\n",
    "# PARSE HORIZON COLUMNS\n",
    "# -------------------------\n",
    "column_frame = [c for c in input_data_converse.columns if c.startswith(\"I_hat(t+\")]\n",
    "# map: column name -> k (เช่น \"I_hat(t+73|t)\" -> 73)\n",
    "col2k = pd.Series(column_frame, index=column_frame).str.extract(r\"I_hat\\(t\\+(\\d+)\\|t\\)\")[0].astype(int).to_dict()\n",
    "\n",
    "# -------------------------\n",
    "# LONG TABLE: (origin -> target)\n",
    "# -------------------------\n",
    "long = input_data_converse.melt(\n",
    "    id_vars=[\"Datetime\", \"I(t)\"], value_vars=column_frame,\n",
    "    var_name=\"col\", value_name=\"yhat\"\n",
    ")\n",
    "long[\"k\"] = long[\"col\"].map(col2k).astype(int)\n",
    "long[\"Datetime_target\"] = long[\"Datetime\"] + pd.to_timedelta(long[\"k\"] * STEP_MIN, unit=\"min\")\n",
    "long = long.rename(columns={\"Datetime\":\"Datetime_origin\", \"I(t)\":\"y_origin\"})\n",
    "long = long[[\"Datetime_origin\", \"Datetime_target\", \"k\", \"yhat\"]]\n",
    "\n",
    "# -------------------------\n",
    "# WIDE (by k): yhat(t|t-k)  for k=1..864\n",
    "# หมายเหตุ: ในเคส origin วันละครั้งที่ 23:55 ตารางนี้จะค่อนข้าง \"sparse\"\n",
    "# -------------------------\n",
    "wide_k = long.pivot_table(index=\"Datetime_target\", columns=\"k\", values=\"yhat\", aggfunc=\"last\")\n",
    "max_k = max(col2k.values())\n",
    "# ให้มีครบทุกคอลัมน์ 1..max_k\n",
    "wide_k = wide_k.reindex(columns=range(1, max_k+1))\n",
    "wide_k.columns = [f\"yhat(t|t-{k})\" for k in wide_k.columns]\n",
    "\n",
    "# แนบค่าจริง y(t) ถ้ามีที่เวลา target (ถ้าไม่มี จะเป็น NaN)\n",
    "# ---- ช่วยประหยัดหน่วยความจำ ----\n",
    "wide_k = wide_k.sort_index()          # เรียงด้วย index (Datetime_target) แทน sort_values ทีหลัง\n",
    "wide_k = wide_k.astype(\"float32\")     # ลด dtype จาก float64 -> float32\n",
    "\n",
    "# เปลี่ยนชื่อคอลัมน์ yhat(...) -> Ihat(...)\n",
    "wide_k.columns = [c.replace(\"yhat(\", \"Ihat(\") for c in wide_k.columns]\n",
    "\n",
    "# ค่าจริงที่เวลา target\n",
    "actual_at_target = (\n",
    "    input_data_converse[[\"Datetime\", \"I(t)\"]]\n",
    "      .rename(columns={\"Datetime\": \"Datetime_target\"})\n",
    "      .set_index(\"Datetime_target\")\n",
    "      .sort_index()\n",
    ")\n",
    "if actual_at_target[\"I(t)\"].dtype != \"float32\":\n",
    "    actual_at_target[\"I(t)\"] = actual_at_target[\"I(t)\"].astype(\"float32\")\n",
    "\n",
    "# join แทน merge + sort_values (ประหยัด RAM มากกว่า)\n",
    "eval_wide = wide_k.join(actual_at_target[[\"I(t)\"]], how=\"outer\")\n",
    "\n",
    "# รีเซ็ต index และจัดคอลัมน์ตามต้องการ\n",
    "eval_wide.index.name = \"Datetime\"\n",
    "eval_wide = eval_wide.reset_index()\n",
    "\n",
    "# ให้มี Ihat(t|t-1)..Ihat(t|t-max_k) ครบและเรียง\n",
    "max_k = max(col2k.values())\n",
    "need_column = [f\"Ihat(t|t-{k})\" for k in range(1, max_k+1)]\n",
    "for name in need_column:\n",
    "    if name not in eval_wide.columns:\n",
    "        eval_wide[name] = np.nan\n",
    "\n",
    "eval_wide = eval_wide[[\"Datetime\", \"I(t)\"] + need_column]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL: version แยกตาม \"วันของ origin\"\n",
    "# (เหมาะมากสำหรับเคส forecast วันละครั้ง 23:55)\n",
    "# จะได้คอลัมน์: yhat_from_origin_D-0, yhat_from_origin_D-1, yhat_from_origin_D-2, yhat_from_origin_D-3\n",
    "# -------------------------\n",
    "tmp = long.copy()\n",
    "tmp[\"origin_day_offset\"] = (tmp[\"Datetime_target\"].dt.normalize() - tmp[\"Datetime_origin\"].dt.normalize()).dt.days\n",
    "format_to_cal = tmp.pivot_table(index=\"Datetime_target\", columns=\"origin_day_offset\", values=\"yhat\", aggfunc=\"last\")\n",
    "\n",
    "# จัดชื่อคอลัมน์ให้อ่านง่าย\n",
    "if format_to_cal is not None and format_to_cal.shape[1] > 0:\n",
    "    new_cols = {}\n",
    "    for off in format_to_cal.columns:\n",
    "        if off == 0:\n",
    "            new_cols[off] = \"yhat_from_origin_D-0\"   # พยากรณ์ที่ทำเมื่อคืน 23:55 ของ \"วันเดียวกัน\"\n",
    "        elif off == 1:\n",
    "            new_cols[off] = \"yhat_from_origin_D-1\"   # ทำที่ 23:55 ของ \"เมื่อวาน\"\n",
    "        elif off == 2:\n",
    "            new_cols[off] = \"yhat_from_origin_D-2\"\n",
    "        elif off == 3:\n",
    "            new_cols[off] = \"yhat_from_origin_D-3\"\n",
    "        else:\n",
    "            new_cols[off] = f\"yhat_from_origin_D-{off}\"\n",
    "    format_to_cal = format_to_cal.rename(columns=new_cols).sort_index(axis=1)\n",
    "\n",
    "# เผื่อ format_to_cal ว่าง (เช่นยังสร้างไม่ได้)\n",
    "if format_to_cal is None or format_to_cal.shape[1] == 0:\n",
    "    eval_by_day = input_data_converse[[\"Datetime\", \"I(t)\"]].rename(columns={\"Datetime\":\"Datetime\"})\n",
    "else:\n",
    "    # จัดการหน่วยความจำ\n",
    "    format_to_cal = format_to_cal.sort_index()\n",
    "    format_to_cal = format_to_cal.astype(\"float32\")\n",
    "\n",
    "    # ใช้ actual_at_target ที่เตรียมไว้ด้านบน (index = Datetime_target)\n",
    "    eval_by_day = format_to_cal.join(actual_at_target[[\"I(t)\"]], how=\"outer\")\n",
    "\n",
    "    eval_by_day.index.name = \"Datetime\"\n",
    "    eval_by_day = eval_by_day.reset_index()\n",
    "\n",
    "# จัดลำดับคอลัมน์: \"Datetime\", \"I(t)\", แล้วตามด้วย yhat_from_origin_D-*\n",
    "front = [\"Datetime\", \"I(t)\"]\n",
    "rest  = [c for c in eval_by_day.columns if c not in front]\n",
    "eval_by_day = eval_by_day[front + rest]\n",
    "\n",
    "eval_by_day = eval_by_day.drop(eval_by_day.index[0]).reset_index(drop=True)\n",
    "eval_wide = eval_wide.drop(eval_wide.index[0]).reset_index(drop=True)\n",
    "eval_wide[\"Datetime\"] = pd.to_datetime(eval_wide[\"Datetime\"])\n",
    "eval_by_day[\"Datetime\"] = pd.to_datetime(eval_by_day[\"Datetime\"])\n",
    "\n",
    "def merge_actuals(input_df):\n",
    "    merged = input_df.merge(\n",
    "        e[[\"Datetime\", \"Irradiance (W/m2)_imputed\"]],\n",
    "        on=\"Datetime\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    merged[\"I(t)\"] = merged[\"Irradiance (W/m2)_imputed\"]\n",
    "    merged = merged.drop(columns=\"Irradiance (W/m2)_imputed\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def rename_day_columns(input_df):\n",
    "    for idx, col in enumerate(input_df.columns[2:], start=1):\n",
    "        input_df.rename(columns={col: f\"day{idx}\"}, inplace=True)\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def replace_negativevalues_with_zero(input_df):\n",
    "    target_cols = [\"day1\", \"day2\", \"day3\"]\n",
    "    for col in target_cols:\n",
    "        mask = (input_df[col] < 1e-4) & input_df[col].notna()\n",
    "        input_df.loc[mask, col] = 0\n",
    "    return input_df\n",
    "\n",
    "eval_wide = merge_actuals(eval_wide)\n",
    "eval_by_day = merge_actuals(eval_by_day)\n",
    "eval_by_day = rename_day_columns(eval_by_day)\n",
    "eval_by_day = replace_negativevalues_with_zero(eval_by_day)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17babee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# convert for Load\n",
    "STEP_MIN = 5  # 1 step = 5 นาที\n",
    "\n",
    "# -------------------------\n",
    "# LOAD\n",
    "# -------------------------\n",
    "input_data_converse = pd.read_csv(\"TimeGPTlag7dayLoad.csv\", parse_dates=[\"Datetime\"])\n",
    "e = pd.read_csv(\"load_feature_5minresample_edited.csv\")\n",
    "e[\"Datetime\"] = pd.to_datetime(e[\"Datetime\"])\n",
    "\n",
    "# -------------------------\n",
    "# PARSE HORIZON COLUMNS\n",
    "# -------------------------\n",
    "column_frame = [c for c in input_data_converse.columns if c.startswith(\"Load_hat(t+\")]\n",
    "# map: column name -> k (เช่น \"I_hat(t+73|t)\" -> 73)\n",
    "col2k = pd.Series(column_frame, index=column_frame).str.extract(r\"Load_hat\\(t\\+(\\d+)\\|t\\)\")[0].astype(int).to_dict()\n",
    "\n",
    "# -------------------------\n",
    "# LONG TABLE: (origin -> target)\n",
    "# -------------------------\n",
    "long = input_data_converse.melt(\n",
    "    id_vars=[\"Datetime\", \"Load(t)\"], value_vars=column_frame,\n",
    "    var_name=\"col\", value_name=\"Ltot\"\n",
    ")\n",
    "long[\"k\"] = long[\"col\"].map(col2k).astype(int)\n",
    "long[\"Datetime_target\"] = long[\"Datetime\"] + pd.to_timedelta(long[\"k\"] * STEP_MIN, unit=\"min\")\n",
    "long = long.rename(columns={\"Datetime\":\"Datetime_origin\", \"Load(t)\":\"y_origin\"})\n",
    "long = long[[\"Datetime_origin\", \"Datetime_target\", \"k\", \"Ltot\"]]\n",
    "\n",
    "# -------------------------\n",
    "# WIDE (by k): Ltot(t|t-k)  for k=1..864\n",
    "# หมายเหตุ: ในเคส origin วันละครั้งที่ 23:55 ตารางนี้จะค่อนข้าง \"sparse\"\n",
    "# -------------------------\n",
    "wide_k = long.pivot_table(index=\"Datetime_target\", columns=\"k\", values=\"Ltot\", aggfunc=\"last\")\n",
    "max_k = max(col2k.values())\n",
    "# ให้มีครบทุกคอลัมน์ 1..max_k\n",
    "wide_k = wide_k.reindex(columns=range(1, max_k+1))\n",
    "wide_k.columns = [f\"Ltot(t|t-{k})\" for k in wide_k.columns]\n",
    "\n",
    "# แนบค่าจริง y(t) ถ้ามีที่เวลา target (ถ้าไม่มี จะเป็น NaN)\n",
    "# ---- ช่วยประหยัดหน่วยความจำ ----\n",
    "wide_k = wide_k.sort_index()          # เรียงด้วย index (Datetime_target) แทน sort_values ทีหลัง\n",
    "wide_k = wide_k.astype(\"float32\")     # ลด dtype จาก float64 -> float32\n",
    "\n",
    "# เปลี่ยนชื่อคอลัมน์ Ltot(...) -> Ihat(...)\n",
    "wide_k.columns = [c.replace(\"Ltot(\", \"Load_hat(\") for c in wide_k.columns]\n",
    "\n",
    "# ค่าจริงที่เวลา target\n",
    "actual_at_target = (\n",
    "    input_data_converse[[\"Datetime\", \"Load(t)\"]]\n",
    "      .rename(columns={\"Datetime\": \"Datetime_target\"})\n",
    "      .set_index(\"Datetime_target\")\n",
    "      .sort_index()\n",
    ")\n",
    "if actual_at_target[\"Load(t)\"].dtype != \"float32\":\n",
    "    actual_at_target[\"Load(t)\"] = actual_at_target[\"Load(t)\"].astype(\"float32\")\n",
    "\n",
    "# join แทน merge + sort_values (ประหยัด RAM มากกว่า)\n",
    "eval_wide = wide_k.join(actual_at_target[[\"Load(t)\"]], how=\"outer\")\n",
    "\n",
    "# รีเซ็ต index และจัดคอลัมน์ตามต้องการ\n",
    "eval_wide.index.name = \"Datetime\"\n",
    "eval_wide = eval_wide.reset_index()\n",
    "\n",
    "# ให้มี Ihat(t|t-1)..Ihat(t|t-max_k) ครบและเรียง\n",
    "max_k = max(col2k.values())\n",
    "need_column = [f\"Load_hat(t|t-{k})\" for k in range(1, max_k+1)]\n",
    "for name in need_column:\n",
    "    if name not in eval_wide.columns:\n",
    "        eval_wide[name] = np.nan\n",
    "\n",
    "eval_wide = eval_wide[[\"Datetime\", \"Load(t)\"] + need_column]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL: version แยกตาม \"วันของ origin\"\n",
    "# (เหมาะมากสำหรับเคส forecast วันละครั้ง 23:55)\n",
    "# จะได้คอลัมน์: yhat_from_origin_D-0, yhat_from_origin_D-1, yhat_from_origin_D-2, yhat_from_origin_D-3\n",
    "# -------------------------\n",
    "tmp = long.copy()\n",
    "tmp[\"origin_day_offset\"] = (tmp[\"Datetime_target\"].dt.normalize() - tmp[\"Datetime_origin\"].dt.normalize()).dt.days\n",
    "format_to_cal = tmp.pivot_table(index=\"Datetime_target\", columns=\"origin_day_offset\", values=\"Ltot\", aggfunc=\"last\")\n",
    "\n",
    "# จัดชื่อคอลัมน์ให้อ่านง่าย\n",
    "if format_to_cal is not None and format_to_cal.shape[1] > 0:\n",
    "    new_cols = {}\n",
    "    for off in format_to_cal.columns:\n",
    "        if off == 0:\n",
    "            new_cols[off] = \"yhat_from_origin_D-0\"   # พยากรณ์ที่ทำเมื่อคืน 23:55 ของ \"วันเดียวกัน\"\n",
    "        elif off == 1:\n",
    "            new_cols[off] = \"yhat_from_origin_D-1\"   # ทำที่ 23:55 ของ \"เมื่อวาน\"\n",
    "        elif off == 2:\n",
    "            new_cols[off] = \"yhat_from_origin_D-2\"\n",
    "        elif off == 3:\n",
    "            new_cols[off] = \"yhat_from_origin_D-3\"\n",
    "        else:\n",
    "            new_cols[off] = f\"yhat_from_origin_D-{off}\"\n",
    "    format_to_cal = format_to_cal.rename(columns=new_cols).sort_index(axis=1)\n",
    "\n",
    "# เผื่อ format_to_cal ว่าง (เช่นยังสร้างไม่ได้)\n",
    "if format_to_cal is None or format_to_cal.shape[1] == 0:\n",
    "    eval_by_day = input_data_converse[[\"Datetime\", \"Load(t)\"]].rename(columns={\"Datetime\":\"Datetime\"})\n",
    "else:\n",
    "    # จัดการหน่วยความจำ\n",
    "    format_to_cal = format_to_cal.sort_index()\n",
    "    format_to_cal = format_to_cal.astype(\"float32\")\n",
    "\n",
    "    # ใช้ actual_at_target ที่เตรียมไว้ด้านบน (index = Datetime_target)\n",
    "    eval_by_day = format_to_cal.join(actual_at_target[[\"Load(t)\"]], how=\"outer\")\n",
    "\n",
    "    eval_by_day.index.name = \"Datetime\"\n",
    "    eval_by_day = eval_by_day.reset_index()\n",
    "\n",
    "# จัดลำดับคอลัมน์: \"Datetime\", \"I(t)\", แล้วตามด้วย yhat_from_origin_D-*\n",
    "front = [\"Datetime\", \"Load(t)\"]\n",
    "rest  = [c for c in eval_by_day.columns if c not in front]\n",
    "eval_by_day = eval_by_day[front + rest]\n",
    "\n",
    "eval_by_day = eval_by_day.drop(eval_by_day.index[0]).reset_index(drop=True)\n",
    "eval_wide = eval_wide.drop(eval_wide.index[0]).reset_index(drop=True)\n",
    "eval_wide[\"Datetime\"] = pd.to_datetime(eval_wide[\"Datetime\"])\n",
    "eval_by_day[\"Datetime\"] = pd.to_datetime(eval_by_day[\"Datetime\"])\n",
    "\n",
    "def merge_actuals(input_df):\n",
    "    merged = input_df.merge(\n",
    "        e[[\"Datetime\", \"Ltot_impute (kW)\"]],\n",
    "        on=\"Datetime\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    merged[\"Load(t)\"] = merged[\"Ltot_impute (kW)\"]\n",
    "    merged = merged.drop(columns=\"Ltot_impute (kW)\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def rename_day_columns(input_df):\n",
    "    for idx, col in enumerate(input_df.columns[2:], start=1):\n",
    "        input_df.rename(columns={col: f\"day{idx}\"}, inplace=True)\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def replace_negativevalues_with_zero(input_df):\n",
    "    target_cols = [\"day1\", \"day2\", \"day3\"]\n",
    "    for col in target_cols:\n",
    "        mask = (input_df[col] < 1e-4) & input_df[col].notna()\n",
    "        input_df.loc[mask, col] = 0\n",
    "    return input_df\n",
    "\n",
    "eval_wide = merge_actuals(eval_wide)\n",
    "eval_by_day = merge_actuals(eval_by_day)\n",
    "eval_by_day = rename_day_columns(eval_by_day)\n",
    "eval_by_day = replace_negativevalues_with_zero(eval_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
